<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>20 Recursive Feature Elimination | The caret Package</title>
  <meta name="description" content="Documentation for the caret package.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="20 Recursive Feature Elimination | The caret Package" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Documentation for the caret package." />
  <meta name="github-repo" content="topepo/caret" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="20 Recursive Feature Elimination | The caret Package" />
  
  <meta name="twitter:description" content="Documentation for the caret package." />
  

<meta name="author" content="Max Kuhn">


<meta name="date" content="2019-03-27">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="feature-selection-using-univariate-filters.html">
<link rel="next" href="feature-selection-using-genetic-algorithms.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.5/datatables.js"></script>
<link href="libs/dt-core-bootstrap-1.10.16/css/dataTables.bootstrap.min.css" rel="stylesheet" />
<link href="libs/dt-core-bootstrap-1.10.16/css/dataTables.bootstrap.extra.css" rel="stylesheet" />
<script src="libs/dt-core-bootstrap-1.10.16/js/jquery.dataTables.min.js"></script>
<script src="libs/dt-core-bootstrap-1.10.16/js/dataTables.bootstrap.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<script src="libs/d3-4.5.0/d3.min.js"></script>
<script src="libs/forceNetwork-binding-0.4/forceNetwork.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.39.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.39.2/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="visualizations.html"><a href="visualizations.html"><i class="fa fa-check"></i><b>2</b> Visualizations</a></li>
<li class="chapter" data-level="3" data-path="pre-processing.html"><a href="pre-processing.html"><i class="fa fa-check"></i><b>3</b> Pre-Processing</a><ul>
<li class="chapter" data-level="3.1" data-path="pre-processing.html"><a href="pre-processing.html#creating-dummy-variables"><i class="fa fa-check"></i><b>3.1</b> Creating Dummy Variables</a></li>
<li class="chapter" data-level="3.2" data-path="pre-processing.html"><a href="pre-processing.html#zero--and-near-zero-variance-predictors"><i class="fa fa-check"></i><b>3.2</b> Zero- and Near Zero-Variance Predictors</a></li>
<li class="chapter" data-level="3.3" data-path="pre-processing.html"><a href="pre-processing.html#identifying-correlated-predictors"><i class="fa fa-check"></i><b>3.3</b> Identifying Correlated Predictors</a></li>
<li class="chapter" data-level="3.4" data-path="pre-processing.html"><a href="pre-processing.html#linear-dependencies"><i class="fa fa-check"></i><b>3.4</b> Linear Dependencies</a></li>
<li class="chapter" data-level="3.5" data-path="pre-processing.html"><a href="pre-processing.html#the-preprocess-function"><i class="fa fa-check"></i><b>3.5</b> The <code>preProcess</code> Function</a></li>
<li class="chapter" data-level="3.6" data-path="pre-processing.html"><a href="pre-processing.html#centering-and-scaling"><i class="fa fa-check"></i><b>3.6</b> Centering and Scaling</a></li>
<li class="chapter" data-level="3.7" data-path="pre-processing.html"><a href="pre-processing.html#imputation"><i class="fa fa-check"></i><b>3.7</b> Imputation</a></li>
<li class="chapter" data-level="3.8" data-path="pre-processing.html"><a href="pre-processing.html#transforming-predictors"><i class="fa fa-check"></i><b>3.8</b> Transforming Predictors</a></li>
<li class="chapter" data-level="3.9" data-path="pre-processing.html"><a href="pre-processing.html#putting-it-all-together"><i class="fa fa-check"></i><b>3.9</b> Putting It All Together</a></li>
<li class="chapter" data-level="3.10" data-path="pre-processing.html"><a href="pre-processing.html#class-distance-calculations"><i class="fa fa-check"></i><b>3.10</b> Class Distance Calculations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-splitting.html"><a href="data-splitting.html"><i class="fa fa-check"></i><b>4</b> Data Splitting</a><ul>
<li class="chapter" data-level="4.1" data-path="data-splitting.html"><a href="data-splitting.html#simple-splitting-based-on-the-outcome"><i class="fa fa-check"></i><b>4.1</b> Simple Splitting Based on the Outcome</a></li>
<li class="chapter" data-level="4.2" data-path="data-splitting.html"><a href="data-splitting.html#splitting-based-on-the-predictors"><i class="fa fa-check"></i><b>4.2</b> Splitting Based on the Predictors</a></li>
<li class="chapter" data-level="4.3" data-path="data-splitting.html"><a href="data-splitting.html#data-splitting-for-time-series"><i class="fa fa-check"></i><b>4.3</b> Data Splitting for Time Series</a></li>
<li class="chapter" data-level="4.4" data-path="data-splitting.html"><a href="data-splitting.html#simple-splitting-with-important-groups"><i class="fa fa-check"></i><b>4.4</b> Simple Splitting with Important Groups</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html"><i class="fa fa-check"></i><b>5</b> Model Training and Tuning</a><ul>
<li class="chapter" data-level="5.1" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#model-training-and-parameter-tuning"><i class="fa fa-check"></i><b>5.1</b> Model Training and Parameter Tuning</a></li>
<li class="chapter" data-level="5.2" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#an-example"><i class="fa fa-check"></i><b>5.2</b> An Example</a></li>
<li class="chapter" data-level="5.3" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#basic-parameter-tuning"><i class="fa fa-check"></i><b>5.3</b> Basic Parameter Tuning</a></li>
<li class="chapter" data-level="5.4" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#notes-on-reproducibility"><i class="fa fa-check"></i><b>5.4</b> Notes on Reproducibility</a></li>
<li class="chapter" data-level="5.5" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#customizing-the-tuning-process"><i class="fa fa-check"></i><b>5.5</b> Customizing the Tuning Process</a><ul>
<li class="chapter" data-level="5.5.1" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#pre-processing-options"><i class="fa fa-check"></i><b>5.5.1</b> Pre-Processing Options</a></li>
<li class="chapter" data-level="5.5.2" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#alternate-tuning-grids"><i class="fa fa-check"></i><b>5.5.2</b> Alternate Tuning Grids</a></li>
<li class="chapter" data-level="5.5.3" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#plotting-the-resampling-profile"><i class="fa fa-check"></i><b>5.5.3</b> Plotting the Resampling Profile</a></li>
<li class="chapter" data-level="5.5.4" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#the-traincontrol-function"><i class="fa fa-check"></i><b>5.5.4</b> The <code>trainControl</code> Function</a></li>
<li class="chapter" data-level="5.5.5" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#alternate-performance-metrics"><i class="fa fa-check"></i><b>5.5.5</b> Alternate Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#choosing-the-final-model"><i class="fa fa-check"></i><b>5.6</b> Choosing the Final Model</a></li>
<li class="chapter" data-level="5.7" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#extracting-predictions-and-class-probabilities"><i class="fa fa-check"></i><b>5.7</b> Extracting Predictions and Class Probabilities</a></li>
<li class="chapter" data-level="5.8" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#exploring-and-comparing-resampling-distributions"><i class="fa fa-check"></i><b>5.8</b> Exploring and Comparing Resampling Distributions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#within-model"><i class="fa fa-check"></i><b>5.8.1</b> Within-Model</a></li>
<li class="chapter" data-level="5.8.2" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#between-models"><i class="fa fa-check"></i><b>5.8.2</b> Between-Models</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#fitting-models-without-parameter-tuning"><i class="fa fa-check"></i><b>5.9</b> Fitting Models Without Parameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="available-models.html"><a href="available-models.html"><i class="fa fa-check"></i><b>6</b> Available Models</a></li>
<li class="chapter" data-level="7" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html"><i class="fa fa-check"></i><b>7</b> <code>train</code> Models By Tag</a><ul>
<li class="chapter" data-level="7.0.1" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#accepts-case-weights"><i class="fa fa-check"></i><b>7.0.1</b> Accepts Case Weights</a></li>
<li class="chapter" data-level="7.0.2" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#bagging"><i class="fa fa-check"></i><b>7.0.2</b> Bagging</a></li>
<li class="chapter" data-level="7.0.3" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#bayesian-model"><i class="fa fa-check"></i><b>7.0.3</b> Bayesian Model</a></li>
<li class="chapter" data-level="7.0.4" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#binary-predictors-only"><i class="fa fa-check"></i><b>7.0.4</b> Binary Predictors Only</a></li>
<li class="chapter" data-level="7.0.5" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#boosting"><i class="fa fa-check"></i><b>7.0.5</b> Boosting</a></li>
<li class="chapter" data-level="7.0.6" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#categorical-predictors-only"><i class="fa fa-check"></i><b>7.0.6</b> Categorical Predictors Only</a></li>
<li class="chapter" data-level="7.0.7" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#cost-sensitive-learning"><i class="fa fa-check"></i><b>7.0.7</b> Cost Sensitive Learning</a></li>
<li class="chapter" data-level="7.0.8" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#discriminant-analysis"><i class="fa fa-check"></i><b>7.0.8</b> Discriminant Analysis</a></li>
<li class="chapter" data-level="7.0.9" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#distance-weighted-discrimination"><i class="fa fa-check"></i><b>7.0.9</b> Distance Weighted Discrimination</a></li>
<li class="chapter" data-level="7.0.10" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#ensemble-model"><i class="fa fa-check"></i><b>7.0.10</b> Ensemble Model</a></li>
<li class="chapter" data-level="7.0.11" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#feature-extraction"><i class="fa fa-check"></i><b>7.0.11</b> Feature Extraction</a></li>
<li class="chapter" data-level="7.0.12" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#feature-selection-wrapper"><i class="fa fa-check"></i><b>7.0.12</b> Feature Selection Wrapper</a></li>
<li class="chapter" data-level="7.0.13" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#gaussian-process"><i class="fa fa-check"></i><b>7.0.13</b> Gaussian Process</a></li>
<li class="chapter" data-level="7.0.14" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#generalized-additive-model"><i class="fa fa-check"></i><b>7.0.14</b> Generalized Additive Model</a></li>
<li class="chapter" data-level="7.0.15" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#generalized-linear-model"><i class="fa fa-check"></i><b>7.0.15</b> Generalized Linear Model</a></li>
<li class="chapter" data-level="7.0.16" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#handle-missing-predictor-data"><i class="fa fa-check"></i><b>7.0.16</b> Handle Missing Predictor Data</a></li>
<li class="chapter" data-level="7.0.17" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#implicit-feature-selection"><i class="fa fa-check"></i><b>7.0.17</b> Implicit Feature Selection</a></li>
<li class="chapter" data-level="7.0.18" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#kernel-method"><i class="fa fa-check"></i><b>7.0.18</b> Kernel Method</a></li>
<li class="chapter" data-level="7.0.19" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#l1-regularization"><i class="fa fa-check"></i><b>7.0.19</b> L1 Regularization</a></li>
<li class="chapter" data-level="7.0.20" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#l2-regularization"><i class="fa fa-check"></i><b>7.0.20</b> L2 Regularization</a></li>
<li class="chapter" data-level="7.0.21" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#linear-classifier"><i class="fa fa-check"></i><b>7.0.21</b> Linear Classifier</a></li>
<li class="chapter" data-level="7.0.22" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#linear-regression"><i class="fa fa-check"></i><b>7.0.22</b> Linear Regression</a></li>
<li class="chapter" data-level="7.0.23" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#logic-regression"><i class="fa fa-check"></i><b>7.0.23</b> Logic Regression</a></li>
<li class="chapter" data-level="7.0.24" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#logistic-regression"><i class="fa fa-check"></i><b>7.0.24</b> Logistic Regression</a></li>
<li class="chapter" data-level="7.0.25" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#mixture-model"><i class="fa fa-check"></i><b>7.0.25</b> Mixture Model</a></li>
<li class="chapter" data-level="7.0.26" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#model-tree"><i class="fa fa-check"></i><b>7.0.26</b> Model Tree</a></li>
<li class="chapter" data-level="7.0.27" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>7.0.27</b> Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="7.0.28" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#neural-network"><i class="fa fa-check"></i><b>7.0.28</b> Neural Network</a></li>
<li class="chapter" data-level="7.0.29" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#oblique-tree"><i class="fa fa-check"></i><b>7.0.29</b> Oblique Tree</a></li>
<li class="chapter" data-level="7.0.30" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#ordinal-outcomes"><i class="fa fa-check"></i><b>7.0.30</b> Ordinal Outcomes</a></li>
<li class="chapter" data-level="7.0.31" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#partial-least-squares"><i class="fa fa-check"></i><b>7.0.31</b> Partial Least Squares</a></li>
<li class="chapter" data-level="7.0.32" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#patient-rule-induction-method"><i class="fa fa-check"></i><b>7.0.32</b> Patient Rule Induction Method</a></li>
<li class="chapter" data-level="7.0.33" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#polynomial-model"><i class="fa fa-check"></i><b>7.0.33</b> Polynomial Model</a></li>
<li class="chapter" data-level="7.0.34" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#prototype-models"><i class="fa fa-check"></i><b>7.0.34</b> Prototype Models</a></li>
<li class="chapter" data-level="7.0.35" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#quantile-regression"><i class="fa fa-check"></i><b>7.0.35</b> Quantile Regression</a></li>
<li class="chapter" data-level="7.0.36" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#radial-basis-function"><i class="fa fa-check"></i><b>7.0.36</b> Radial Basis Function</a></li>
<li class="chapter" data-level="7.0.37" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#random-forest"><i class="fa fa-check"></i><b>7.0.37</b> Random Forest</a></li>
<li class="chapter" data-level="7.0.38" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#regularization"><i class="fa fa-check"></i><b>7.0.38</b> Regularization</a></li>
<li class="chapter" data-level="7.0.39" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#relevance-vector-machines"><i class="fa fa-check"></i><b>7.0.39</b> Relevance Vector Machines</a></li>
<li class="chapter" data-level="7.0.40" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#ridge-regression"><i class="fa fa-check"></i><b>7.0.40</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.0.41" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#robust-methods"><i class="fa fa-check"></i><b>7.0.41</b> Robust Methods</a></li>
<li class="chapter" data-level="7.0.42" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#robust-model"><i class="fa fa-check"></i><b>7.0.42</b> Robust Model</a></li>
<li class="chapter" data-level="7.0.43" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#roc-curves"><i class="fa fa-check"></i><b>7.0.43</b> ROC Curves</a></li>
<li class="chapter" data-level="7.0.44" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#rule-based-model"><i class="fa fa-check"></i><b>7.0.44</b> Rule-Based Model</a></li>
<li class="chapter" data-level="7.0.45" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#self-organising-maps"><i class="fa fa-check"></i><b>7.0.45</b> Self-Organising Maps</a></li>
<li class="chapter" data-level="7.0.46" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#string-kernel"><i class="fa fa-check"></i><b>7.0.46</b> String Kernel</a></li>
<li class="chapter" data-level="7.0.47" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#support-vector-machines"><i class="fa fa-check"></i><b>7.0.47</b> Support Vector Machines</a></li>
<li class="chapter" data-level="7.0.48" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#supports-class-probabilities"><i class="fa fa-check"></i><b>7.0.48</b> Supports Class Probabilities</a></li>
<li class="chapter" data-level="7.0.49" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#text-mining"><i class="fa fa-check"></i><b>7.0.49</b> Text Mining</a></li>
<li class="chapter" data-level="7.0.50" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#tree-based-model"><i class="fa fa-check"></i><b>7.0.50</b> Tree-Based Model</a></li>
<li class="chapter" data-level="7.0.51" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#two-class-only"><i class="fa fa-check"></i><b>7.0.51</b> Two Class Only</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="models-clustered-by-tag-similarity.html"><a href="models-clustered-by-tag-similarity.html"><i class="fa fa-check"></i><b>8</b> Models Clustered by Tag Similarity</a></li>
<li class="chapter" data-level="9" data-path="parallel-processing.html"><a href="parallel-processing.html"><i class="fa fa-check"></i><b>9</b> Parallel Processing</a></li>
<li class="chapter" data-level="10" data-path="random-hyperparameter-search.html"><a href="random-hyperparameter-search.html"><i class="fa fa-check"></i><b>10</b> Random Hyperparameter Search</a></li>
<li class="chapter" data-level="11" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html"><i class="fa fa-check"></i><b>11</b> Subsampling For Class Imbalances</a><ul>
<li class="chapter" data-level="11.1" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#subsampling-techniques"><i class="fa fa-check"></i><b>11.1</b> Subsampling Techniques</a></li>
<li class="chapter" data-level="11.2" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#subsampling-during-resampling"><i class="fa fa-check"></i><b>11.2</b> Subsampling During Resampling</a></li>
<li class="chapter" data-level="11.3" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#complications"><i class="fa fa-check"></i><b>11.3</b> Complications</a></li>
<li class="chapter" data-level="11.4" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#using-custom-subsampling-techniques"><i class="fa fa-check"></i><b>11.4</b> Using Custom Subsampling Techniques</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html"><i class="fa fa-check"></i><b>12</b> Using Recipes with train</a><ul>
<li class="chapter" data-level="12.1" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html#why-should-you-learn-this"><i class="fa fa-check"></i><b>12.1</b> Why Should you learn this?</a><ul>
<li class="chapter" data-level="12.1.1" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html#more-versatile-tools-for-preprocessing-data"><i class="fa fa-check"></i><b>12.1.1</b> More versatile tools for preprocessing data</a></li>
<li class="chapter" data-level="12.1.2" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html#using-additional-data-to-measure-performance"><i class="fa fa-check"></i><b>12.1.2</b> Using additional data to measure performance</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html#an-example-1"><i class="fa fa-check"></i><b>12.2</b> An Example</a></li>
<li class="chapter" data-level="12.3" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html#case-weights"><i class="fa fa-check"></i><b>12.3</b> Case Weights</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html"><i class="fa fa-check"></i><b>13</b> Using Your Own Model in <code>train</code></a><ul>
<li class="chapter" data-level="13.1" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#introduction-1"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-1-svms-with-laplacian-kernels"><i class="fa fa-check"></i><b>13.2</b> Illustrative Example 1: SVMs with Laplacian Kernels</a></li>
<li class="chapter" data-level="13.3" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#model-components"><i class="fa fa-check"></i><b>13.3</b> Model Components</a><ul>
<li class="chapter" data-level="13.3.1" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-parameters-element"><i class="fa fa-check"></i><b>13.3.1</b> The parameters Element</a></li>
<li class="chapter" data-level="13.3.2" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-grid-element"><i class="fa fa-check"></i><b>13.3.2</b> The <code>grid</code> Element</a></li>
<li class="chapter" data-level="13.3.3" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-fit-element"><i class="fa fa-check"></i><b>13.3.3</b> The <code>fit</code> Element</a></li>
<li class="chapter" data-level="13.3.4" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-predict-element"><i class="fa fa-check"></i><b>13.3.4</b> The <code>predict</code> Element</a></li>
<li class="chapter" data-level="13.3.5" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-prob-element"><i class="fa fa-check"></i><b>13.3.5</b> The <code>prob</code> Element</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-sort-element"><i class="fa fa-check"></i><b>13.4</b> The sort Element</a><ul>
<li class="chapter" data-level="13.4.1" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-levels-element"><i class="fa fa-check"></i><b>13.4.1</b> The <code>levels</code> Element</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-2-something-more-complicated---logitboost"><i class="fa fa-check"></i><b>13.5</b> Illustrative Example 2: Something More Complicated - <code>LogitBoost</code></a></li>
<li class="chapter" data-level="13.6" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-3-nonstandard-formulas"><i class="fa fa-check"></i><b>13.6</b> Illustrative Example 3: Nonstandard Formulas</a></li>
<li class="chapter" data-level="13.7" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-4-pls-feature-extraction-pre-processing"><i class="fa fa-check"></i><b>13.7</b> Illustrative Example 4: PLS Feature Extraction Pre-Processing</a></li>
<li class="chapter" data-level="13.8" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-5-optimizing-probability-thresholds-for-class-imbalances"><i class="fa fa-check"></i><b>13.8</b> Illustrative Example 5: Optimizing probability thresholds for class imbalances</a></li>
<li class="chapter" data-level="13.9" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-6-offsets-in-generalized-linear-models"><i class="fa fa-check"></i><b>13.9</b> Illustrative Example 6: Offsets in Generalized Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="adaptive-resampling.html"><a href="adaptive-resampling.html"><i class="fa fa-check"></i><b>14</b> Adaptive Resampling</a></li>
<li class="chapter" data-level="15" data-path="variable-importance.html"><a href="variable-importance.html"><i class="fa fa-check"></i><b>15</b> Variable Importance</a><ul>
<li class="chapter" data-level="15.1" data-path="variable-importance.html"><a href="variable-importance.html#model-specific-metrics"><i class="fa fa-check"></i><b>15.1</b> Model Specific Metrics</a></li>
<li class="chapter" data-level="15.2" data-path="variable-importance.html"><a href="variable-importance.html#model-independent-metrics"><i class="fa fa-check"></i><b>15.2</b> Model Independent Metrics</a></li>
<li class="chapter" data-level="15.3" data-path="variable-importance.html"><a href="variable-importance.html#an-example-2"><i class="fa fa-check"></i><b>15.3</b> An Example</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html"><i class="fa fa-check"></i><b>16</b> Miscellaneous Model Functions</a><ul>
<li class="chapter" data-level="16.1" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#yet-another-k-nearest-neighbor-function"><i class="fa fa-check"></i><b>16.1</b> Yet Another <em>k</em>-Nearest Neighbor Function</a></li>
<li class="chapter" data-level="16.2" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#partial-least-squares-discriminant-analysis"><i class="fa fa-check"></i><b>16.2</b> Partial Least Squares Discriminant Analysis</a></li>
<li class="chapter" data-level="16.3" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#bagged-mars-and-fda"><i class="fa fa-check"></i><b>16.3</b> Bagged MARS and FDA</a></li>
<li class="chapter" data-level="16.4" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#bagging-1"><i class="fa fa-check"></i><b>16.4</b> Bagging</a><ul>
<li class="chapter" data-level="16.4.1" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#the-fit-function"><i class="fa fa-check"></i><b>16.4.1</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="16.4.2" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#the-pred-function"><i class="fa fa-check"></i><b>16.4.2</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="16.4.3" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#the-aggregate-function"><i class="fa fa-check"></i><b>16.4.3</b> The <code>aggregate</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#model-averaged-neural-networks"><i class="fa fa-check"></i><b>16.5</b> Model Averaged Neural Networks</a></li>
<li class="chapter" data-level="16.6" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#neural-networks-with-a-principal-component-step"><i class="fa fa-check"></i><b>16.6</b> Neural Networks with a Principal Component Step</a></li>
<li class="chapter" data-level="16.7" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#independent-component-regression"><i class="fa fa-check"></i><b>16.7</b> Independent Component Regression</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>17</b> Measuring Performance</a><ul>
<li class="chapter" data-level="17.1" data-path="measuring-performance.html"><a href="measuring-performance.html#measures-for-regression"><i class="fa fa-check"></i><b>17.1</b> Measures for Regression</a></li>
<li class="chapter" data-level="17.2" data-path="measuring-performance.html"><a href="measuring-performance.html#measures-for-predicted-classes"><i class="fa fa-check"></i><b>17.2</b> Measures for Predicted Classes</a></li>
<li class="chapter" data-level="17.3" data-path="measuring-performance.html"><a href="measuring-performance.html#measures-for-class-probabilities"><i class="fa fa-check"></i><b>17.3</b> Measures for Class Probabilities</a></li>
<li class="chapter" data-level="17.4" data-path="measuring-performance.html"><a href="measuring-performance.html#lift-curves"><i class="fa fa-check"></i><b>17.4</b> Lift Curves</a></li>
<li class="chapter" data-level="17.5" data-path="measuring-performance.html"><a href="measuring-performance.html#calibration-curves"><i class="fa fa-check"></i><b>17.5</b> Calibration Curves</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html"><i class="fa fa-check"></i><b>18</b> Feature Selection Overview</a><ul>
<li class="chapter" data-level="18.1" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html#models-with-built-in-feature-selection"><i class="fa fa-check"></i><b>18.1</b> Models with Built-In Feature Selection</a></li>
<li class="chapter" data-level="18.2" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html#feature-selection-methods"><i class="fa fa-check"></i><b>18.2</b> Feature Selection Methods</a></li>
<li class="chapter" data-level="18.3" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html#external-validation"><i class="fa fa-check"></i><b>18.3</b> External Validation</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html"><i class="fa fa-check"></i><b>19</b> Feature Selection using Univariate Filters</a><ul>
<li class="chapter" data-level="19.1" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#univariate-filters"><i class="fa fa-check"></i><b>19.1</b> Univariate Filters</a></li>
<li class="chapter" data-level="19.2" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#basic-syntax"><i class="fa fa-check"></i><b>19.2</b> Basic Syntax</a><ul>
<li class="chapter" data-level="19.2.1" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-score-function"><i class="fa fa-check"></i><b>19.2.1</b> The <code>score</code> Function</a></li>
<li class="chapter" data-level="19.2.2" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-filter-function"><i class="fa fa-check"></i><b>19.2.2</b> The <code>filter</code> Function</a></li>
<li class="chapter" data-level="19.2.3" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-fit-function-1"><i class="fa fa-check"></i><b>19.2.3</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="19.2.4" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-summary-and-pred-functions"><i class="fa fa-check"></i><b>19.2.4</b> The <code>summary</code> and <code>pred</code> Functions</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#fexample"><i class="fa fa-check"></i><b>19.3</b> The Example</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html"><i class="fa fa-check"></i><b>20</b> Recursive Feature Elimination</a><ul>
<li class="chapter" data-level="20.1" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#backwards-selection"><i class="fa fa-check"></i><b>20.1</b> Backwards Selection</a></li>
<li class="chapter" data-level="20.2" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#resampling-and-external-validation"><i class="fa fa-check"></i><b>20.2</b> Resampling and External Validation</a></li>
<li class="chapter" data-level="20.3" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#recursive-feature-elimination-via-caret"><i class="fa fa-check"></i><b>20.3</b> Recursive Feature Elimination via <a href="http://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a></a></li>
<li class="chapter" data-level="20.4" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#rfeexample"><i class="fa fa-check"></i><b>20.4</b> An Example</a></li>
<li class="chapter" data-level="20.5" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#rfehelpers"><i class="fa fa-check"></i><b>20.5</b> Helper Functions</a><ul>
<li class="chapter" data-level="20.5.1" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-summary-function"><i class="fa fa-check"></i><b>20.5.1</b> The <code>summary</code> Function</a></li>
<li class="chapter" data-level="20.5.2" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-fit-function-2"><i class="fa fa-check"></i><b>20.5.2</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="20.5.3" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-pred-function-1"><i class="fa fa-check"></i><b>20.5.3</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="20.5.4" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-rank-function"><i class="fa fa-check"></i><b>20.5.4</b> The <code>rank</code> Function</a></li>
<li class="chapter" data-level="20.5.5" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-selectsize-function"><i class="fa fa-check"></i><b>20.5.5</b> The <code>selectSize</code> Function</a></li>
<li class="chapter" data-level="20.5.6" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-selectvar-function"><i class="fa fa-check"></i><b>20.5.6</b> The <code>selectVar</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="20.6" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#rfeexample2"><i class="fa fa-check"></i><b>20.6</b> The Example</a></li>
<li class="chapter" data-level="20.7" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#rferecipes"><i class="fa fa-check"></i><b>20.7</b> Using a Recipe</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html"><i class="fa fa-check"></i><b>21</b> Feature Selection using Genetic Algorithms</a><ul>
<li class="chapter" data-level="21.1" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#genetic-algorithms"><i class="fa fa-check"></i><b>21.1</b> Genetic Algorithms</a></li>
<li class="chapter" data-level="21.2" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#internal-and-external-performance-estimates"><i class="fa fa-check"></i><b>21.2</b> Internal and External Performance Estimates</a></li>
<li class="chapter" data-level="21.3" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#basic-syntax-1"><i class="fa fa-check"></i><b>21.3</b> Basic Syntax</a></li>
<li class="chapter" data-level="21.4" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#gaexample"><i class="fa fa-check"></i><b>21.4</b> Genetic Algorithm Example</a></li>
<li class="chapter" data-level="21.5" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#customizing-the-search"><i class="fa fa-check"></i><b>21.5</b> Customizing the Search</a><ul>
<li class="chapter" data-level="21.5.1" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-fit-function-3"><i class="fa fa-check"></i><b>21.5.1</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="21.5.2" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-pred-function-2"><i class="fa fa-check"></i><b>21.5.2</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="21.5.3" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-fitness_intern-function"><i class="fa fa-check"></i><b>21.5.3</b> The <code>fitness_intern</code> Function</a></li>
<li class="chapter" data-level="21.5.4" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-fitness_extern-function"><i class="fa fa-check"></i><b>21.5.4</b> The <code>fitness_extern</code> Function</a></li>
<li class="chapter" data-level="21.5.5" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-initial-function"><i class="fa fa-check"></i><b>21.5.5</b> The <code>initial</code> Function</a></li>
<li class="chapter" data-level="21.5.6" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-selection-function"><i class="fa fa-check"></i><b>21.5.6</b> The <code>selection</code> Function</a></li>
<li class="chapter" data-level="21.5.7" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-crossover-function"><i class="fa fa-check"></i><b>21.5.7</b> The <code>crossover</code> Function</a></li>
<li class="chapter" data-level="21.5.8" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-mutation-function"><i class="fa fa-check"></i><b>21.5.8</b> The <code>mutation</code> Function</a></li>
<li class="chapter" data-level="21.5.9" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-selectiter-function"><i class="fa fa-check"></i><b>21.5.9</b> The <code>selectIter</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-example-revisited"><i class="fa fa-check"></i><b>21.6</b> The Example Revisited</a></li>
<li class="chapter" data-level="21.7" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#using-recipes"><i class="fa fa-check"></i><b>21.7</b> Using Recipes</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html"><i class="fa fa-check"></i><b>22</b> Feature Selection using Simulated Annealing</a><ul>
<li class="chapter" data-level="22.1" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#simulated-annealing"><i class="fa fa-check"></i><b>22.1</b> Simulated Annealing</a></li>
<li class="chapter" data-level="22.2" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#internal-and-external-performance-estimates-1"><i class="fa fa-check"></i><b>22.2</b> Internal and External Performance Estimates</a></li>
<li class="chapter" data-level="22.3" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#basic-syntax-2"><i class="fa fa-check"></i><b>22.3</b> Basic Syntax</a></li>
<li class="chapter" data-level="22.4" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#saexample"><i class="fa fa-check"></i><b>22.4</b> Simulated Annealing Example</a></li>
<li class="chapter" data-level="22.5" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#customizing-the-search-1"><i class="fa fa-check"></i><b>22.5</b> Customizing the Search</a><ul>
<li class="chapter" data-level="22.5.1" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-fit-function-4"><i class="fa fa-check"></i><b>22.5.1</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="22.5.2" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-pred-function-3"><i class="fa fa-check"></i><b>22.5.2</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="22.5.3" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-fitness_intern-function-1"><i class="fa fa-check"></i><b>22.5.3</b> The <code>fitness_intern</code> Function</a></li>
<li class="chapter" data-level="22.5.4" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-fitness_extern-function-1"><i class="fa fa-check"></i><b>22.5.4</b> The <code>fitness_extern</code> Function</a></li>
<li class="chapter" data-level="22.5.5" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-initial-function-1"><i class="fa fa-check"></i><b>22.5.5</b> The <code>initial</code> Function</a></li>
<li class="chapter" data-level="22.5.6" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-perturb-function"><i class="fa fa-check"></i><b>22.5.6</b> The <code>perturb</code> Function</a></li>
<li class="chapter" data-level="22.5.7" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-prob-function"><i class="fa fa-check"></i><b>22.5.7</b> The <code>prob</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="22.6" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#using-recipes-1"><i class="fa fa-check"></i><b>22.6</b> Using Recipes</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="data-sets.html"><a href="data-sets.html"><i class="fa fa-check"></i><b>23</b> Data Sets</a><ul>
<li class="chapter" data-level="23.1" data-path="data-sets.html"><a href="data-sets.html#blood-brain-barrier-data"><i class="fa fa-check"></i><b>23.1</b> Blood-Brain Barrier Data</a></li>
<li class="chapter" data-level="23.2" data-path="data-sets.html"><a href="data-sets.html#cox-2-activity-data"><i class="fa fa-check"></i><b>23.2</b> COX-2 Activity Data</a></li>
<li class="chapter" data-level="23.3" data-path="data-sets.html"><a href="data-sets.html#dhfr-inhibition"><i class="fa fa-check"></i><b>23.3</b> DHFR Inhibition</a></li>
<li class="chapter" data-level="23.4" data-path="data-sets.html"><a href="data-sets.html#tecator-nir-data"><i class="fa fa-check"></i><b>23.4</b> Tecator NIR Data</a></li>
<li class="chapter" data-level="23.5" data-path="data-sets.html"><a href="data-sets.html#fatty-acid-composition-data"><i class="fa fa-check"></i><b>23.5</b> Fatty Acid Composition Data</a></li>
<li class="chapter" data-level="23.6" data-path="data-sets.html"><a href="data-sets.html#german-credit-data"><i class="fa fa-check"></i><b>23.6</b> German Credit Data</a></li>
<li class="chapter" data-level="23.7" data-path="data-sets.html"><a href="data-sets.html#kelly-blue-book"><i class="fa fa-check"></i><b>23.7</b> Kelly Blue Book</a></li>
<li class="chapter" data-level="23.8" data-path="data-sets.html"><a href="data-sets.html#cell-body-segmentation-data"><i class="fa fa-check"></i><b>23.8</b> Cell Body Segmentation Data</a></li>
<li class="chapter" data-level="23.9" data-path="data-sets.html"><a href="data-sets.html#sacramento-house-price-data"><i class="fa fa-check"></i><b>23.9</b> Sacramento House Price Data</a></li>
<li class="chapter" data-level="23.10" data-path="data-sets.html"><a href="data-sets.html#animal-scat-data"><i class="fa fa-check"></i><b>23.10</b> Animal Scat Data</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="session-information.html"><a href="session-information.html"><i class="fa fa-check"></i><b>24</b> Session Information</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The <code>caret</code> Package</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="recursive-feature-elimination" class="section level1">
<h1><span class="header-section-number">20</span> Recursive Feature Elimination</h1>
<p>Contents</p>
<ul>
<li><a href="#search">Feature Selection Using Search Algorithms</a></li>
<li><a href="model-training-and-tuning.html#resamp">Resampling and External Validation</a></li>
<li><a href="recursive-feature-elimination.html#rfe">Recursive Feature Elimination via <code>caret</code></a></li>
<li><a href="recursive-feature-elimination.html#rfeexample">An Example</a></li>
<li><a href="recursive-feature-elimination.html#rfehelpers">Helper Functions</a></li>
<li><a href="recursive-feature-elimination.html#rfeexample2">The Example</a></li>
<li><a href="##rferecipes">Using a Recipe</a></li>
</ul>
<div id="backwards-selection" class="section level2">
<h2><span class="header-section-number">20.1</span> Backwards Selection</h2>
<p>First, the algorithm fits the model to all predictors. Each predictor is ranked using it’s importance to the model. Let <em>S</em> be a sequence of ordered numbers which are candidate values for the number of predictors to retain (<em>S<sub>1</sub></em> &gt; <em>S<sub>2</sub></em>, …). At each iteration of feature selection, the <em>S<sub>i</sub></em> top ranked predictors are retained, the model is refit and performance is assessed. The value of <em>S<sub>i</sub></em> with the best performance is determined and the top <em>S<sub>i</sub></em> predictors are used to fit the final model. Algorithm 1 has a more complete definition.</p>
<p>The algorithm has an optional step (line 1.9) where the predictor rankings are recomputed on the model on the reduced feature set. <a href="http://rd.springer.com/chapter/10.1007%2F978-3-540-25966-4_33">Svetnik <em>et al</em> (2004)</a> showed that, for random forest models, there was a decrease in performance when the rankings were re-computed at every step. However, in other cases when the initial rankings are not good (e.g. linear models with highly collinear predictors), re-calculation can slightly improve performance.</p>
<p><img src="premade/Algo1.png" /><!-- --></p>
<p>One potential issue over-fitting to the predictor set such that the wrapper procedure could focus on nuances of the training data that are not found in future samples (i.e. over-fitting to predictors and samples).</p>
<p>For example, suppose a very large number of uninformative predictors were collected and one such predictor randomly correlated with the outcome. The RFE algorithm would give a good rank to this variable and the prediction error (on the same data set) would be lowered. It would take a different test/validation to find out that this predictor was uninformative. The was referred to as “selection bias” by <a href="http://www.pnas.org/content/99/10/6562.short">Ambroise and McLachlan (2002)</a>.</p>
<p>In the current RFE algorithm, the training data is being used for at least three purposes: predictor selection, model fitting and performance evaluation. Unless the number of samples is large, especially in relation to the number of variables, one static training set may not be able to fulfill these needs.</p>
<div id="resamp">

</div>
</div>
<div id="resampling-and-external-validation" class="section level2">
<h2><span class="header-section-number">20.2</span> Resampling and External Validation</h2>
<p>Since feature selection is part of the model building process, resampling methods (e.g. cross-validation, the bootstrap) should factor in the variability caused by feature selection when calculating performance. For example, the RFE procedure in Algorithm 1 can estimate the model performance on line 1.7, which during the selection process. <a href="http://www.pnas.org/content/99/10/6562.short">Ambroise and McLachlan (2002)</a> and <a href="http://rd.springer.com/chapter/10.1007%2F978-3-540-25966-4_33">Svetnik <em>et al</em> (2004)</a> showed that improper use of resampling to measure performance will result in models that perform poorly on new samples.</p>
<p>To get performance estimates that incorporate the variation due to feature selection, it is suggested that the steps in Algorithm 1 be encapsulated inside an outer layer of resampling (e.g. 10-fold cross-validation). Algorithm 2 shows a version of the algorithm that uses resampling.</p>
<p>While this will provide better estimates of performance, it is more computationally burdensome. For users with access to machines with multiple processors, the first <code>For</code> loop in Algorithm 2 (line 2.1) can be easily parallelized. Another complication to using resampling is that multiple lists of the “best” predictors are generated at each iteration. At first this may seem like a disadvantage, but it does provide a more probabilistic assessment of predictor importance than a ranking based on a single fixed data set. At the end of the algorithm, a consensus ranking can be used to determine the best predictors to retain.</p>
<p><img src="premade/Algo2.png" /><!-- --></p>
<div id="rfe">

</div>
</div>
<div id="recursive-feature-elimination-via-caret" class="section level2">
<h2><span class="header-section-number">20.3</span> Recursive Feature Elimination via <a href="http://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a></h2>
<p>In <a href="http://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a>, Algorithm 1 is implemented by the function <code>rfeIter</code>. The resampling-based Algorithm 2 is in the <code>rfe</code> function. Given the potential selection bias issues, this document focuses on <code>rfe</code>. There are several arguments:</p>
<ul>
<li><code>x</code>, a matrix or data frame of predictor variables</li>
<li><code>y</code>, a vector (numeric or factor) of outcomes</li>
<li><code>sizes</code>, a integer vector for the specific subset sizes that should be tested (which need not to include <code>ncol(x)</code>)</li>
<li><code>rfeControl</code>, a list of options that can be used to specify the model and the methods for prediction, ranking etc.</li>
</ul>
<p>For a specific model, a set of functions must be specified in <code>rfeControl$functions</code>. Sections below has descriptions of these sub-functions. There are a number of pre-defined sets of functions for several models, including: linear regression (in the object <code>lmFuncs</code>), random forests (<code>rfFuncs</code>), naive Bayes (<code>nbFuncs</code>), bagged trees (<code>treebagFuncs</code>) and functions that can be used with <a href="http://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a>’s <code>train</code> function (<code>caretFuncs</code>). The latter is useful if the model has tuning parameters that must be determined at each iteration.</p>
<div id="rfeexample">

</div>
</div>
<div id="rfeexample" class="section level2">
<h2><span class="header-section-number">20.4</span> An Example</h2>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="kw">library</span>(mlbench)
<span class="kw">library</span>(Hmisc)
<span class="kw">library</span>(randomForest)</code></pre>
<p>To test the algorithm, the “Friedman 1” benchmark (Friedman, 1991) was used. There are five informative variables generated by the equation</p>
<p><img src="premade/FEq.png" /><!-- --></p>
<p>In the simulation used here:</p>
<pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">100</span>
p &lt;-<span class="st"> </span><span class="dv">40</span>
sigma &lt;-<span class="st"> </span><span class="dv">1</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)
sim &lt;-<span class="st"> </span><span class="kw">mlbench.friedman1</span>(n, <span class="dt">sd =</span> sigma)
<span class="kw">colnames</span>(sim<span class="op">$</span>x) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">paste</span>(<span class="st">&quot;real&quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>),
                     <span class="kw">paste</span>(<span class="st">&quot;bogus&quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>))
bogus &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>p), <span class="dt">nrow =</span> n)
<span class="kw">colnames</span>(bogus) &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;bogus&quot;</span>, <span class="dv">5</span><span class="op">+</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(bogus)), <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)
x &lt;-<span class="st"> </span><span class="kw">cbind</span>(sim<span class="op">$</span>x, bogus)
y &lt;-<span class="st"> </span>sim<span class="op">$</span>y</code></pre>
<p>Of the 50 predictors, there are 45 pure noise variables: 5 are uniform on <span class="math display">\[0, 1\]</span> and 40 are random univariate standard normals. The predictors are centered and scaled:</p>
<pre class="sourceCode r"><code class="sourceCode r">normalization &lt;-<span class="st"> </span><span class="kw">preProcess</span>(x)
x &lt;-<span class="st"> </span><span class="kw">predict</span>(normalization, x)
x &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(x)
subsets &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>, <span class="dv">25</span>)</code></pre>
<p>The simulation will fit models with subset sizes of 25, 20, 15, 10, 5, 4, 3, 2, 1.</p>
<p>As previously mentioned, to fit linear models, the <code>lmFuncs</code> set of functions can be used. To do this, a control object is created with the <code>rfeControl</code> function. We also specify that repeated 10-fold cross-validation should be used in line 2.1 of Algorithm 2. The number of folds can be changed via the <code>number</code> argument to <code>rfeControl</code> (defaults to 10). The <code>verbose</code> option prevents copious amounts of output from being produced.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">10</span>)

ctrl &lt;-<span class="st"> </span><span class="kw">rfeControl</span>(<span class="dt">functions =</span> lmFuncs,
                   <span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,
                   <span class="dt">repeats =</span> <span class="dv">5</span>,
                   <span class="dt">verbose =</span> <span class="ot">FALSE</span>)

lmProfile &lt;-<span class="st"> </span><span class="kw">rfe</span>(x, y,
                 <span class="dt">sizes =</span> subsets,
                 <span class="dt">rfeControl =</span> ctrl)

lmProfile</code></pre>
<pre><code>## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) 
## 
## Resampling performance over subset size:
## 
##  Variables  RMSE Rsquared   MAE RMSESD RsquaredSD  MAESD Selected
##          1 3.950   0.3790 3.381 0.6379     0.2149 0.5867         
##          2 3.552   0.4985 3.000 0.5820     0.2007 0.5807         
##          3 3.069   0.6107 2.593 0.6022     0.1582 0.5588         
##          4 2.889   0.6658 2.319 0.8208     0.1969 0.5852        *
##          5 2.949   0.6566 2.349 0.8012     0.1856 0.5599         
##         10 3.252   0.5965 2.628 0.8256     0.1781 0.6016         
##         15 3.405   0.5712 2.709 0.8862     0.1985 0.6603         
##         20 3.514   0.5562 2.799 0.9162     0.2048 0.7334         
##         25 3.700   0.5313 2.987 0.9095     0.1972 0.7500         
##         50 4.067   0.4756 3.268 0.8819     0.1908 0.7315         
## 
## The top 4 variables (out of 4):
##    real4, real5, real2, real1</code></pre>
<p>The output shows that the best subset size was estimated to be 4 predictors. This set includes informative variables but did not include them all. The <code>predictors</code> function can be used to get a text string of variable names that were picked in the final model. The <code>lmProfile</code> is a list of class <code>&quot;rfe&quot;</code> that contains an object <code>fit</code> that is the final linear model with the remaining terms. The model can be used to get predictions for future or test samples.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predictors</span>(lmProfile)</code></pre>
<pre><code>## [1] &quot;real4&quot; &quot;real5&quot; &quot;real2&quot; &quot;real1&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">lmProfile<span class="op">$</span>fit</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ ., data = tmp)
## 
## Coefficients:
## (Intercept)        real4        real5        real2        real1  
##      14.613        2.857        1.965        1.625        1.359</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(lmProfile<span class="op">$</span>resample)</code></pre>
<pre><code>##    Variables     RMSE  Rsquared      MAE    Resample
## 4          4 1.923763 0.9142474 1.640438 Fold01.Rep1
## 14         4 2.212266 0.8403133 1.845878 Fold02.Rep1
## 24         4 4.074172 0.5052766 3.095980 Fold03.Rep1
## 34         4 3.938895 0.3250410 2.992700 Fold04.Rep1
## 44         4 3.311426 0.6652186 2.195083 Fold05.Rep1
## 54         4 2.286320 0.6974626 1.840118 Fold06.Rep1</code></pre>
<p>There are also several plot methods to visualize the results. <code>plot(lmProfile)</code> produces the performance profile across different subset sizes, as shown in the figure below.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">trellis.par.set</span>(<span class="kw">caretTheme</span>())
<span class="kw">plot</span>(lmProfile, <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;g&quot;</span>, <span class="st">&quot;o&quot;</span>))</code></pre>
<p><img src="rfe/rfe_lmprofile-1.svg" width="672" /></p>
<p>Also the resampling results are stored in the sub-object <code>lmProfile$resample</code> and can be used with several lattice functions. Univariate lattice functions (<code>densityplot</code>, <code>histogram</code>) can be used to plot the resampling distribution while bivariate functions (<code>xyplot</code>, <code>stripplot</code>) can be used to plot the distributions for different subset sizes. In the latter case, the option <code>returnResamp`` = &quot;all&quot;</code> in <code>rfeControl</code> can be used to save all the resampling results. Example images are shown below for the random forest model.</p>
<div id="rfehelpers">

</div>
</div>
<div id="rfehelpers" class="section level2">
<h2><span class="header-section-number">20.5</span> Helper Functions</h2>
<p>To use feature elimination for an arbitrary model, a set of functions must be passed to <code>rfe</code> for each of the steps in Algorithm 2.</p>
<p>This section defines those functions and uses the existing random forest functions as an illustrative example. <a href="http://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a> contains a list called <code>rfFuncs</code>, but this document will use a more simple version that will be better for illustrating the ideas. A set of simplified functions used here and called <code>rfRFE</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">rfRFE &lt;-<span class="st">  </span><span class="kw">list</span>(<span class="dt">summary =</span> defaultSummary,
               <span class="dt">fit =</span> <span class="cf">function</span>(x, y, first, last, ...){
                 <span class="kw">library</span>(randomForest)
                 <span class="kw">randomForest</span>(x, y, <span class="dt">importance =</span> first, ...)
                 },
               <span class="dt">pred =</span> <span class="cf">function</span>(object, x)  <span class="kw">predict</span>(object, x),
               <span class="dt">rank =</span> <span class="cf">function</span>(object, x, y) {
                 vimp &lt;-<span class="st"> </span><span class="kw">varImp</span>(object)
                 vimp &lt;-<span class="st"> </span>vimp[<span class="kw">order</span>(vimp<span class="op">$</span>Overall,<span class="dt">decreasing =</span> <span class="ot">TRUE</span>),,drop =<span class="st"> </span><span class="ot">FALSE</span>]
                 vimp<span class="op">$</span>var &lt;-<span class="st"> </span><span class="kw">rownames</span>(vimp)                  
                 vimp
                 },
               <span class="dt">selectSize =</span> pickSizeBest,
               <span class="dt">selectVar =</span> pickVars)</code></pre>
<div id="the-summary-function" class="section level3">
<h3><span class="header-section-number">20.5.1</span> The <code>summary</code> Function</h3>
<p>The <code>summary</code> function takes the observed and predicted values and computes one or more performance metrics (see line 2.14). The input is a data frame with columns <code>obs</code> and <code>pred</code>. The output should be a named vector of numeric variables. Note that the <code>metric</code> argument of the <code>rfe</code> function should reference one of the names of the output of <code>summary</code>. The example function is:</p>
<pre class="sourceCode r"><code class="sourceCode r">rfRFE<span class="op">$</span>summary</code></pre>
<pre><code>## function (data, lev = NULL, model = NULL) 
## {
##     if (is.character(data$obs)) 
##         data$obs &lt;- factor(data$obs, levels = lev)
##     postResample(data[, &quot;pred&quot;], data[, &quot;obs&quot;])
## }
## &lt;bytecode: 0x7fa726eefe08&gt;
## &lt;environment: namespace:caret&gt;</code></pre>
<p>Two functions in <a href="http://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a> that can be used as the summary funciton are <code>defaultSummary</code> and <code>twoClassSummary</code> (for classification problems with two classes).</p>
</div>
<div id="the-fit-function-2" class="section level3">
<h3><span class="header-section-number">20.5.2</span> The <code>fit</code> Function</h3>
<p>This function builds the model based on the current data set (lines 2.3, 2.9 and 2.17). The arguments for the function must be:</p>
<ul>
<li><code>x</code>: the current training set of predictor data with the appropriate subset of variables</li>
<li><code>y</code>: the current outcome data (either a numeric or factor vector)</li>
<li><code>first</code>: a single logical value for whether the current predictor set has all possible variables (e.g. line 2.3)</li>
<li><code>last</code>: similar to <code>first</code>, but <code>TRUE</code> when the last model is fit with the final subset size and predictors. (line 2.17)</li>
<li><code>...</code>: optional arguments to pass to the fit function in the call to <code>rfe</code></li>
</ul>
<p>The function should return a model object that can be used to generate predictions. For random forest, the fit function is simple:</p>
<pre class="sourceCode r"><code class="sourceCode r">rfRFE<span class="op">$</span>fit</code></pre>
<pre><code>## function(x, y, first, last, ...){
##                  library(randomForest)
##                  randomForest(x, y, importance = first, ...)
##                  }</code></pre>
<p>For feature selection without re-ranking at each iteration, the random forest variable importances only need to be computed on the first iterations when all of the predictors are in the model. This can be accomplished using <code>importance`` = first</code>.</p>
</div>
<div id="the-pred-function-1" class="section level3">
<h3><span class="header-section-number">20.5.3</span> The <code>pred</code> Function</h3>
<p>This function returns a vector of predictions (numeric or factors) from the current model (lines 2.4 and 2.10). The input arguments must be</p>
<ul>
<li><code>object</code>: the model generated by the <code>fit</code> function</li>
<li><code>x</code>: the current set of predictor set for the held-back samples</li>
</ul>
<p>For random forests, the function is a simple wrapper for the predict function:</p>
<pre class="sourceCode r"><code class="sourceCode r">rfRFE<span class="op">$</span>pred</code></pre>
<pre><code>## function(object, x)  predict(object, x)</code></pre>
<p>For classification, it is probably a good idea to ensure that the resulting factor variables of predictions has the same levels as the input data.</p>
</div>
<div id="the-rank-function" class="section level3">
<h3><span class="header-section-number">20.5.4</span> The <code>rank</code> Function</h3>
<p>This function is used to return the predictors in the order of the most important to the least important (lines 2.5 and 2.11). Inputs are:</p>
<ul>
<li><code>object</code>: the model generated by the <code>fit</code> function</li>
<li><code>x</code>: the current set of predictor set for the training samples</li>
<li><code>y</code>: the current training outcomes</li>
</ul>
<p>The function should return a data frame with a column called <code>var</code> that has the current variable names. The first row should be the most important predictor etc. Other columns can be included in the output and will be returned in the final <code>rfe</code> object.</p>
<p>For random forests, the function below uses <a href="http://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a>’s <code>varImp</code> function to extract the random forest importances and orders them. For classification, <code>randomForest</code> will produce a column of importances for each class. In this case, the default ranking function orders the predictors by the averages importance across the classes.</p>
<pre class="sourceCode r"><code class="sourceCode r">rfRFE<span class="op">$</span>rank</code></pre>
<pre><code>## function(object, x, y) {
##                  vimp &lt;- varImp(object)
##                  vimp &lt;- vimp[order(vimp$Overall,decreasing = TRUE),,drop = FALSE]
##                  vimp$var &lt;- rownames(vimp)                  
##                  vimp
##                  }</code></pre>
</div>
<div id="the-selectsize-function" class="section level3">
<h3><span class="header-section-number">20.5.5</span> The <code>selectSize</code> Function</h3>
<p>This function determines the optimal number of predictors based on the resampling output (line 2.15). Inputs for the function are:</p>
<ul>
<li><code>x</code>: a matrix with columns for the performance metrics and the number of variables, called <code>Variables</code></li>
<li><code>metric</code>: a character string of the performance measure to optimize (e.g. RMSE, Accuracy)</li>
<li><code>maximize</code>: a single logical for whether the metric should be maximized</li>
</ul>
<p>This function should return an integer corresponding to the optimal subset size.</p>
<p><a href="http://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a> comes with two examples functions for this purpose: <code>pickSizeBest</code> and <code>pickSizeTolerance</code>. The former simply selects the subset size that has the best value. The latter takes into account the whole profile and tries to pick a subset size that is small without sacrificing too much performance. For example, suppose we have computed the RMSE over a series of variables sizes:</p>
<pre class="sourceCode r"><code class="sourceCode r">example &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">RMSE =</span> <span class="kw">c</span>(<span class="fl">3.215</span>, <span class="fl">2.819</span>, <span class="fl">2.414</span>, <span class="fl">2.144</span>, 
                               <span class="fl">2.014</span>, <span class="fl">1.997</span>, <span class="fl">2.025</span>, <span class="fl">1.987</span>, 
                               <span class="fl">1.971</span>, <span class="fl">2.055</span>, <span class="fl">1.935</span>, <span class="fl">1.999</span>, 
                               <span class="fl">2.047</span>, <span class="fl">2.002</span>, <span class="fl">1.895</span>, <span class="fl">2.018</span>),
                               <span class="dt">Variables =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">16</span>)</code></pre>
<p>These are depicted in the figure below. The solid circle identifies the subset size with the absolute smallest RMSE. However, there are many smaller subsets that produce approximately the same performance but with fewer predictors. In this case, we might be able to accept a slightly larger error for less predictors.</p>
<p>The <code>pickSizeTolerance</code> determines the absolute best value then the percent difference of the other points to this value. In the case of RMSE, this would be</p>
<p><img src="premade/tol.png" /><!-- --></p>
<p>where <em>RMSE<sub>{opt}</sub></em> is the absolute best error rate. These “tolerance” values are plotted in the bottom panel. The solid triangle is the smallest subset size that is within 10% of the optimal value.</p>
<p>This approach can produce good results for many of the tree based models, such as random forest, where there is a plateau of good performance for larger subset sizes. For trees, this is usually because unimportant variables are infrequently used in splits and do not significantly affect performance.</p>
<pre class="sourceCode r"><code class="sourceCode r">## Find the row with the absolute smallest RMSE
smallest &lt;-<span class="st"> </span><span class="kw">pickSizeBest</span>(example, <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>, <span class="dt">maximize =</span> <span class="ot">FALSE</span>)
smallest</code></pre>
<pre><code>## [1] 15</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">## Now one that is within 10% of the smallest
within10Pct &lt;-<span class="st"> </span><span class="kw">pickSizeTolerance</span>(example, <span class="dt">metric =</span> <span class="st">&quot;RMSE&quot;</span>, <span class="dt">tol =</span> <span class="dv">10</span>, <span class="dt">maximize =</span> <span class="ot">FALSE</span>)
within10Pct</code></pre>
<pre><code>## [1] 5</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">minRMSE &lt;-<span class="st"> </span><span class="kw">min</span>(example<span class="op">$</span>RMSE)
example<span class="op">$</span>Tolerance &lt;-<span class="st"> </span>(example<span class="op">$</span>RMSE <span class="op">-</span><span class="st"> </span>minRMSE)<span class="op">/</span>minRMSE <span class="op">*</span><span class="st"> </span><span class="dv">100</span>   

## Plot the profile and the subsets selected using the 
## two different criteria

<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">2</span>))

<span class="kw">plot</span>(example<span class="op">$</span>Variables[<span class="op">-</span><span class="kw">c</span>(smallest, within10Pct)], 
     example<span class="op">$</span>RMSE[<span class="op">-</span><span class="kw">c</span>(smallest, within10Pct)],
     <span class="dt">ylim =</span> <span class="kw">extendrange</span>(example<span class="op">$</span>RMSE),
     <span class="dt">ylab =</span> <span class="st">&quot;RMSE&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Variables&quot;</span>)

<span class="kw">points</span>(example<span class="op">$</span>Variables[smallest], 
       example<span class="op">$</span>RMSE[smallest], <span class="dt">pch =</span> <span class="dv">16</span>, <span class="dt">cex=</span> <span class="fl">1.3</span>)

<span class="kw">points</span>(example<span class="op">$</span>Variables[within10Pct], 
       example<span class="op">$</span>RMSE[within10Pct], <span class="dt">pch =</span> <span class="dv">17</span>, <span class="dt">cex=</span> <span class="fl">1.3</span>)
 
<span class="kw">with</span>(example, <span class="kw">plot</span>(Variables, Tolerance))
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">10</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</code></pre>
<p><img src="rfe/rfe_lmdens-1.svg" width="672" /></p>
</div>
<div id="the-selectvar-function" class="section level3">
<h3><span class="header-section-number">20.5.6</span> The <code>selectVar</code> Function</h3>
<p>After the optimal subset size is determined, this function will be used to calculate the best rankings for each variable across all the resampling iterations (line 2.16). Inputs for the function are:</p>
<ul>
<li><code>y</code>: a list of variables importance for each resampling iteration and each subset size (generated by the user-defined <code>rank</code> function). In the example, each each of the cross-validation groups the output of the <span class="mx funCall">rank</span> function is saved for each of the 10 subset sizes (including the original subset). If the rankings are not recomputed at each iteration, the values will be the same within each cross-validation iteration.</li>
<li><code>size</code>: the integer returned by the <code>selectSize</code> function</li>
</ul>
<p>This function should return a character string of predictor names (of length <code>size</code>) in the order of most important to least important</p>
<p>For random forests, only the first importance calculation (line 2.5) is used since these are the rankings on the full set of predictors. These importances are averaged and the top predictors are returned.</p>
<pre class="sourceCode r"><code class="sourceCode r">rfRFE<span class="op">$</span>selectVar</code></pre>
<pre><code>## function (y, size) 
## {
##     finalImp &lt;- ddply(y[, c(&quot;Overall&quot;, &quot;var&quot;)], .(var), function(x) mean(x$Overall, 
##         na.rm = TRUE))
##     names(finalImp)[2] &lt;- &quot;Overall&quot;
##     finalImp &lt;- finalImp[order(finalImp$Overall, decreasing = TRUE), 
##         ]
##     as.character(finalImp$var[1:size])
## }
## &lt;bytecode: 0x7fa6f06cdc18&gt;
## &lt;environment: namespace:caret&gt;</code></pre>
<p>Note that if the predictor rankings are recomputed at each iteration (line 2.11) the user will need to write their own selection function to use the other ranks.</p>
<div id="rfeexample2">

</div>
</div>
</div>
<div id="rfeexample2" class="section level2">
<h2><span class="header-section-number">20.6</span> The Example</h2>
<p>For random forest, we fit the same series of model sizes as the linear model. The option to save all the resampling results across subset sizes was changed for this model and are used to show the lattice plot function capabilities in the figures below.</p>
<pre class="sourceCode r"><code class="sourceCode r">ctrl<span class="op">$</span>functions &lt;-<span class="st"> </span>rfRFE
ctrl<span class="op">$</span>returnResamp &lt;-<span class="st"> &quot;all&quot;</span>
<span class="kw">set.seed</span>(<span class="dv">10</span>)
rfProfile &lt;-<span class="st"> </span><span class="kw">rfe</span>(x, y, <span class="dt">sizes =</span> subsets, <span class="dt">rfeControl =</span> ctrl)
rfProfile</code></pre>
<pre><code>## 
## Recursive feature selection
## 
## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) 
## 
## Resampling performance over subset size:
## 
##  Variables  RMSE Rsquared   MAE RMSESD RsquaredSD  MAESD Selected
##          1 4.667   0.2159 3.907 0.8779    0.20591 0.7889         
##          2 3.801   0.4082 3.225 0.5841    0.21832 0.5858         
##          3 3.157   0.6005 2.650 0.5302    0.14847 0.5156         
##          4 2.696   0.7646 2.277 0.4044    0.08625 0.3962        *
##          5 2.859   0.7553 2.385 0.4577    0.10529 0.4382         
##         10 3.061   0.7184 2.570 0.4378    0.13898 0.4106         
##         15 3.170   0.7035 2.671 0.4423    0.15140 0.4110         
##         20 3.327   0.6826 2.812 0.4469    0.16074 0.4117         
##         25 3.356   0.6729 2.843 0.4634    0.16947 0.4324         
##         50 3.525   0.6437 3.011 0.4597    0.17207 0.4196         
## 
## The top 4 variables (out of 4):
##    real4, real5, real2, real1</code></pre>
<p>The resampling profile can be visualized along with plots of the individual resampling results:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">trellis.par.set</span>(<span class="kw">caretTheme</span>())
plot1 &lt;-<span class="st"> </span><span class="kw">plot</span>(rfProfile, <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;g&quot;</span>, <span class="st">&quot;o&quot;</span>))
plot2 &lt;-<span class="st"> </span><span class="kw">plot</span>(rfProfile, <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;g&quot;</span>, <span class="st">&quot;o&quot;</span>), <span class="dt">metric =</span> <span class="st">&quot;Rsquared&quot;</span>)
<span class="kw">print</span>(plot1, <span class="dt">split=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">more=</span><span class="ot">TRUE</span>)
<span class="kw">print</span>(plot2, <span class="dt">split=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>))</code></pre>
<p><img src="rfe/rfe_rf_plot1-1.svg" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r">plot1 &lt;-<span class="st"> </span><span class="kw">xyplot</span>(rfProfile, 
                <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;g&quot;</span>, <span class="st">&quot;p&quot;</span>, <span class="st">&quot;smooth&quot;</span>), 
                <span class="dt">ylab =</span> <span class="st">&quot;RMSE CV Estimates&quot;</span>)
plot2 &lt;-<span class="st"> </span><span class="kw">densityplot</span>(rfProfile, 
                     <span class="dt">subset =</span> Variables <span class="op">&lt;</span><span class="st"> </span><span class="dv">5</span>, 
                     <span class="dt">adjust =</span> <span class="fl">1.25</span>, 
                     <span class="dt">as.table =</span> <span class="ot">TRUE</span>, 
                     <span class="dt">xlab =</span> <span class="st">&quot;RMSE CV Estimates&quot;</span>, 
                     <span class="dt">pch =</span> <span class="st">&quot;|&quot;</span>)
<span class="kw">print</span>(plot1, <span class="dt">split=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">more=</span><span class="ot">TRUE</span>)
<span class="kw">print</span>(plot2, <span class="dt">split=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>))</code></pre>
<p><img src="rfe/rfe_rf_plot2-1.svg" width="672" /></p>
<div id="rferecipes">

</div>
</div>
<div id="rferecipes" class="section level2">
<h2><span class="header-section-number">20.7</span> Using a Recipe</h2>
<p>A recipe can be used to specify the model terms and any preprocessing that may be needed. Instead of using</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rfe</span>(<span class="dt">x =</span> predictors, <span class="dt">y =</span> outcome)</code></pre>
<p>an existing recipe can be used along with a data frame containing the predictors and outcome:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rfe</span>(recipe, data)</code></pre>
<p>The recipe is prepped within each resample in the same manner that <code>train</code> executes the <code>preProc</code> option. However, since a recipe can do a variety of different operations, there are some potentially complicating factors. The main pitfall is that the recipe can involve the creation and deletion of predictors. There are a number of steps that can reduce the number of predictors, such as the ones for pooling factors into an “other” category, PCA signal extraction, as well as filters for near-zero variance predictors and highly correlated predictors. For this reason, it may be difficult to know how many predictors are available for the full model. Also, this number will likely vary between iterations of resampling.</p>
<p>To illustrate, let’s use the blood-brain barrier data where there is a high degree of correlation between the predictors. A simple recipe could be</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(recipes)
<span class="kw">library</span>(tidyverse)

<span class="kw">data</span>(BloodBrain)

<span class="co"># combine into a single data frame</span>
bbb &lt;-<span class="st"> </span>bbbDescr
bbb<span class="op">$</span>y &lt;-<span class="st"> </span>logBBB

bbb_rec &lt;-<span class="st"> </span><span class="kw">recipe</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> bbb) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">step_center</span>(<span class="kw">all_predictors</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">step_scale</span>(<span class="kw">all_predictors</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">step_nzv</span>(<span class="kw">all_predictors</span>()) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">step_pca</span>(<span class="kw">all_predictors</span>(), <span class="dt">threshold =</span> <span class="fl">.95</span>) </code></pre>
<p>Originally, there are 134 predictors and, for the entire data set, the processed version has:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prep</span>(bbb_rec, <span class="dt">training =</span> bbb, <span class="dt">retain =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">juice</span>(<span class="kw">all_predictors</span>()) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ncol</span>()</code></pre>
<pre><code>## [1] 28</code></pre>
<p>When calling <code>rfe</code>, let’s start the maximum subset size at 28:</p>
<pre class="sourceCode r"><code class="sourceCode r">bbb_ctrl &lt;-<span class="st"> </span><span class="kw">rfeControl</span>(
  <span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,
  <span class="dt">repeats =</span> <span class="dv">5</span>,
  <span class="dt">functions =</span> lmFuncs, 
  <span class="dt">returnResamp =</span> <span class="st">&quot;all&quot;</span>
)

<span class="kw">set.seed</span>(<span class="dv">36</span>)
lm_rfe &lt;-<span class="st"> </span><span class="kw">rfe</span>(
  bbb_rec,
  <span class="dt">data =</span> bbb,
  <span class="dt">sizes =</span> <span class="dv">2</span><span class="op">:</span><span class="dv">28</span>,
  <span class="dt">rfeControl =</span> bbb_ctrl
)

<span class="kw">ggplot</span>(lm_rfe) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</code></pre>
<p><img src="rfe/rfe-rec-1.svg" width="672" /></p>
<p>What was the distribution of the maximum number of terms:</p>
<pre class="sourceCode r"><code class="sourceCode r">term_dist &lt;-<span class="st"> </span>
<span class="st">  </span>lm_rfe<span class="op">$</span>resample <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Resample) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarize</span>(<span class="dt">max_terms =</span> <span class="kw">max</span>(Variables))
<span class="kw">table</span>(term_dist<span class="op">$</span>max_terms)</code></pre>
<pre><code>## 
## 27 28 29 
##  7 40  3</code></pre>
<p>So… 28ish.</p>
<p>Suppose that we used <code>sizes = 2:ncol(bbbDescr)</code> when calling <code>rfe</code>. A warning is issued that:</p>
<pre><code>Warning message:
For the training set, the recipe generated fewer predictors than the 130 expected 
in `sizes` and the number of subsets will be truncated to be &lt;= 28 </code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="feature-selection-using-univariate-filters.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="feature-selection-using-genetic-algorithms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
